import mysql.connector
import pandas as pd
import glob, time

#Connecting to MySQL DB - local host
conn = mysql.connector.connect(
    host = "x8autxobia7sgh74.cbetxkdyhwsb.us-east-1.rds.amazonaws.com",
    user = "ocodf96j5psl63w2",
    password = "sp56qfrb5yqhxg5o",
    database = "xojsllvnlgwf8y03"
)
#Create cursor
cursor = conn.cursor()

#Load CSVs
all_files = glob.glob("CSVs/PlayerGeneral*.csv")

#Concatenate all files into one
df = pd.concat([pd.read_csv(f, on_bad_lines='skip') for f in all_files]) 

#Change nan values to None
df = df.where((pd.notnull(df)), None)
df = df.fillna(0)

#Truncate table before reloading
cursor.execute("TRUNCATE TABLE xojsllvnlgwf8y03.player_info")

#loop through the data frame
for i,row in df.iterrows():
    #Skip the empty rows from scraping header rows off of FBRef
    if row[0]:
        if len(row[0]) <= 4:
            continue
    
    #Trim characters off filename
    if (row[4]):
        start_index = row[4].find('"') + 1
        end_index = row[4].find('"', start_index)
        trim_photo = row[4][start_index:end_index]


    with open("./Scraping/photos/" + trim_photo, "rb") as image_file:
        row[4] = image_file.read()

    sql = "INSERT INTO xojsllvnlgwf8y03.player_info VALUES (%s,%s,%s,%s,%s)"
    cursor.execute(sql, tuple(row))
    
    print("Record inserted")
    conn.commit()

conn.close()


